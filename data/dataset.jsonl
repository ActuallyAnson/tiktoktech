{"title": "https://en.wikipedia.org/wiki/Digital_Services_Act", "summary": "The Digital Services Act (DSA) is an EU regulation adopted in 2022 that addresses illegal content, transparent advertising and disinformation. It updates the Electronic Commerce Directive 2000 in EU law, and was proposed alongside the Digital Markets Act (DMA). The DSA applies to online platforms and intermediaries such as social networks, marketplaces, pornographic platforms, and app stores. Key requirements include disclosing to regulators how their algorithms work, providing users with explanations for content moderation decisions, and implementing stricter controls on targeted advertising. It also imposes specific rules on \"very large\" online platforms and search engines (those having more than 45 million monthly active users in the EU).", "content": "The Digital Services Act (DSA) is an EU regulation adopted in 2022 that addresses illegal content, transparent advertising and disinformation. It updates the Electronic Commerce Directive 2000 in EU law, and was proposed alongside the Digital Markets Act (DMA). The DSA applies to online platforms and intermediaries such as social networks, marketplaces, pornographic platforms, and app stores. Key requirements include disclosing to regulators how their algorithms work, providing users with explanations for content moderation decisions, and implementing stricter controls on targeted advertising. It also imposes specific rules on \"very large\" online platforms and search engines (those having more than 45 million monthly active users in the EU).\n\nUrsula von der Leyen proposed a \"new Digital Services Act\" in her 2019 bid for the European Commission's presidency. The expressed purpose of the DSA was to update the European Union's legal framework for illegal content on intermediaries, in particular by modernising the e-Commerce Directive that had been adopted in 2000. In doing so, the DSA aimed to harmonise different national laws in the European Union that have emerged to address illegal content at national level. Most prominent amongst these laws was the German NetzDG, and similar laws in Austria (\"Kommunikationsplattformen-Gesetz\") and France (\"Loi Avia\"). With the adoption of the Digital Services Act at European level, those national laws were planned to be overridden and would have to be amended. In practice, this would lead to new legislation regarding illegal content, transparent advertising and disinformation.\n\nThe DSA is meant to \"govern the content moderation practices of social media platforms\" and address illegal content. It is organised in five chapters, with the most important chapters regulating the liability exemption of intermediaries (Chapter 2), the obligations on intermediaries (Chapter 3), and the cooperation and enforcement framework between the commission and national authorities (Chapter 4). The DSA proposal maintains the current rule according to which companies that host others' data become liable when informed that this data is illegal. This so-called \"conditional liability exemption\" is fundamentally different from the broad immunities given to intermediaries under the equivalent rule (\"Section 230 CDA\") in the United States. The DSA applies to intermediary service providers that offer their services to users based in the European Union, irrespective of whether the intermediary service provider is established in the European Union. In addition to the liability exemptions, the DSA would introduce a wide-ranging set of new obligations on platforms, including some that aim to disclose to regulators how their algorithms work, while other obligations would create transparency on how decisions to remove content are taken and on the way advertisers target users. The European Centre for Algorithmic Transparency was created to aid the enforcement of this. A December 2020 Time article said that while many of its provisions only apply to platforms which have more than 45 million users in the European Union, the Act could have repercussions beyond Europe. Platforms including Facebook, Twitter, TikTok, and Google's subsidiary YouTube would meet that threshold and be subjected to the new obligations. A 16 November 2021 Internet Policy Review listed some of new obligations including mandatory \"notice-and-action\" requirements, for example, respect fundamental rights, mandatory redress for content removal decisions, and a comprehensive risk management and audit framework. Companies that do not comply with the new obligations risk fines of up to 6% on their global annual turnover. In addition, the Commission can apply periodic penalties up to 5% of the average daily worldwide turnover for each day of delay in complying with remedies, interim measures, and commitments. As a last resort measure, if the infringement persists and causes serious harm to users and entails criminal offences involving threat to persons' life or safety, the Commission can request the temporary suspension of the service.\n\nOn 23 April 2023, the European Commission named a first list of 19 online platforms that will be required to comply starting 25 August 2023. They include the following very large online platforms (VLOPs) with more than 45 million monthly active users in the EU as of 17 February 2023: Alibaba, AliExpress, Amazon Store, Apple AppStore, Booking.com, Facebook, Google Play, Google Maps, Google Shopping, Instagram, LinkedIn, Pinterest, PornHub (added 20 December 2023), Shein (added 26 April 2024), Snapchat, Stripchat (added 20 December 2023), Temu (added 31 May 2024), TikTok, Wikipedia, X (formerly Twitter), XNXX (added 10 July 2024), XVideos (added 20 December 2023), YouTube, Zalando. Very Large Online Search Engines (VLOSEs): Bing, Google Search. Amazon and Zalando both initiated proceedings in the General Court challenging the designations, claiming unequal treatment compared to other large retailers, and that their core business models are retail not distributing third party content/products. Zalando argued the criteria and methodology lack transparency, for instance in how it counts active users, while Amazon said VLOP rules are disproportionate for its business model and asked to be exempted from transparency around targeted ads. As of December 2023, 13 VLOPs have received a request for information (RFI), the procedure necessary to verify compliance with the DSA, and one is being subjected to a formal proceedings. 3 further platforms, all of them providing adult content, were added on 20 December 2023.\n\nThe European Commission submitted the DSA alongside the Digital Markets Act (DMA) to the European Parliament and the Council on 15 December 2020. The DSA was prepared by von der Leyen Commission members Margrethe Vestager (Executive Vice President of the European Commission for A Europe Fit for the Digital Age) and Thierry Breton (European Commissioner for Internal Market). The Digital Services Act builds in large parts on the non-binding Commission Recommendation 2018/314 of 1 March 2018 when it comes to illegal content on platforms. However, it goes further in addressing topics such as disinformation and other risks especially on very large online platforms. As part of the preparatory phase, the European Commission launched a public consultation on the package to gather evidence between July and September 2020. An impact assessment was published alongside the proposal on 15 December 2020 with the relevant evidence base. The European Parliament appointed Danish Social Democrat Christel Schaldemose as rapporteur for the Digital Services Act. On 20 January 2022 the Parliament voted to introduce amendments in the DSA for tracking-free advertising and a ban on using a minor's data for targeted ads, as well as a new right for users to seek compensation for damages. In the wake of the Facebook Files revelations and a hearing by Facebook Whistleblower Frances Haugen in the European Parliament, the European Parliament also strengthened the rules on fighting disinformation and harmful content, as well as tougher auditing requirements. The Council of the European Union adopted its position on 25 November 2021. The most significant changes introduced by the Member States are to entrust the European Commission with the enforcement of the new rules, in the wake of allegations and complaints that the Irish Data Protection Watchdog was not effectively policing the bloc's data protection rules against platform companies. The Data Governance Act (DGA) was formally approved by the European Parliament on 6 April 2022. This sets up a legal framework for common data spaces in Europe which will increase data sharing in sectors such as finance, health, and the environment. With Russia using social media platforms to spread misinformation about the 2022 Russian invasion of Ukraine, European policymakers felt a greater sense of urgency to move the legislation forward to ensure that major tech platforms were transparent and properly regulated, according to The Washington Post. On 22 April 2022, the Council of the European Union and the European Parliament reached a deal on the Digital Services Act in Brussels following sixteen hours of negotiations. According to The Washington Post, the agreement reached in Brussels solidifies the two-bill plan—the Digital Services Act and the Digital Markets Act, a law regulating competition. The latter is aimed at preventing abuse of power against smaller competitors by larger \"gatekeepers\". On 5 July 2022, the European Parliament approved both the DSA and the DMA. Following this, on 4 October 2022, the European Council gave its final approval to the DSA. The DSA was adopted on 19 October 2022 and was published in the Official Journal of the European Union on 27 October 2022. It came into force on 16 November 2022. Most services were given 15 months to comply with its provisions (until 17 February 2024). However, \"very large\" online platforms and search engines, after their designation as such, had only four months to comply (until 23 August 2023).\n\nThe DSA was passed alongside the Digital Markets Act and the Democracy Action Plan. The latter of these is focused on addressing the nuanced legal interpretation of free speech on digital platforms, a fundamental right that has been extensively guided by the European Court of Human Rights (ECtHR) and the European Convention on Human Rights. Accordingly, the Democracy Action Plan, and subsequently the DSA, were strongly influenced by the Delfi AS v. Estonia and Magyar Tartalomszolgáltatók Egyesülete and Index.hu Zrt v. Hungary ECtHR cases, which outlined a framework for assessing intermediary liability on digital platforms. In Delfi AS v. Estonia, the ECtHR applied proportionality analysis when considering whether the Estonian courts' decision to hold the online platform Delfi liable for hate speech posted by its users was a proportionate restriction on Delfi's right to freedom of expression. The court found that, given the serious nature of the hate speech, the Estonian courts' actions were justified to protect the rights of others. In other words, the ECtHR upheld the liability of online platforms for hate speech posted by their users, underlining that platforms could be expected to take proactive steps to control content when there is a clear risk of harm from unlawful comments. This case highlighted the responsibilities of platforms to prevent the spread of harmful content. On the other hand, the MTE and Index.hu v. Hungary case illustrated the nuanced limits of freedom of speech on digital platforms. In its application of proportionality analysis, the ECtHR found that the Hungarian courts had failed to strike a fair balance between protecting reputation and ensuring freedom of expression. The Hungarian courts imposed strict liability on the platforms for user comments that were offensive but did not constitute hate speech, constituting a disproportionate interference in the platforms' right to freedom of expression. The ECtHR ruled that imposing strict liability on platforms for user comments, without consideration of the nature of the comments or the context in which they were made, could infringe on freedom of expression. This judgment emphasized the need for a balance between protecting reputation and upholding free speech on digital platforms. These decisions by the ECtHR provided critical legal precedents that shaped the EU's decision-making process on the framework of the DSA. In particular, the DSA drew from the ECtHR's distinction between different types of illegal content, as well as its proportionality analysis in both cases, by incorporating nuanced rules on intermediary liability and ensuring that measures taken by platforms do not unreasonably restrict users' freedom of expression and information.\n\nMedia reactions to the Digital Services Act have been mixed. In January 2022, the editorial board of The Washington Post stated that the U.S. could learn from these rules, while whistleblower Frances Haugen stated that it could set a \"gold standard\" of regulation worldwide. Tech journalist Casey Newton has argued that the DSA will shape US tech policy. Mike Masnick of Techdirt praised the DSA for ensuring the right to pay for digital services anonymously, but criticised the act for not including provisions that would have required a court order for the removal of illegal content. Scholars have begun critically examining the Digital Services Act. Some academics have expressed concerns that the Digital Services Act might be too rigid and prescribed, excessively focused on individual content decisions or vague risk assessments. Civil Society organisations such as Electronic Effront Foundation have called for stronger privacy protections. Human Rights Watch has welcomed the transparency and user remedies but called for an end to abusive surveillance and profiling. Amnesty International has welcomed many aspects of the proposal in terms of fundamental rights balance, but also asked for further restrictions on advertising. Advocacy organisation Avaaz has compared the Digital Services Act to the Paris Agreement for climate change. Following the 2023 Hamas-led attack on Israel, Thierry Breton wrote public letters to X, Meta Platforms, TikTok, and YouTube on how their platforms complied with the DSA regarding content related to the conflict and upcoming elections. The Atlantic Council's Digital Forensic Research Lab reported that Breton's letters did not follow DSA processes, and digital rights group Access Now criticised Breton's letters for drawing a \"false equivalence\" between illegal content and disinformation. Tech companies have repeatedly criticised the heavy burden of the rules and the alleged lack of clarity of the Digital Services Act, and have been accused of lobbying to undermine some of the more far-reaching demands by law-makers, notably on bans for targeted advertising, and a high-profile apology from Sundar Pichai to Breton on leaked plans by Google to lobby against the Digital Services Act. A bipartisan group of US senators have called the DSA and DMA discriminatory, claiming that the legislation would \"focus on regulations on a handful of American companies while failing to regulate similar companies based in Europe, China, Russia and elsewhere.\" The DSA was mostly welcomed by the European media sector. Due to the influence gatekeepers have in selecting and controlling the visibility of certain journalistic articles over others through their online platforms, the European Federation of Journalists encouraged EU legislators to further increase the transparency of platforms' recommendation systems via the DSA. Nevertheless, the DSA's later stage inter-institutional negotiations, or trilogues, have been criticized as lacking transparency and equitable participation. These criticisms mirror past experiences with the drafting of the EU Regulation on Preventing the Dissemination of Terrorist Content Online as well as the General Data Protection Regulation (GDPR). Swedish member of the European Parliament Jessica Stegrud argued that the DSA's focus on preventing the spread of disinformation and \"harmful content\" would undermine freedom of speech. After the first round of the 2024 Romanian presidential election was invalidated due to reports allegedly showing Russian involvement on TikTok in favor of Călin Georgescu, an investigation was conducted to determine whether TikTok had breached the DSA.\n\nIn August 2024, TikTok agreed to withdraw its TikTok Lite rewards feature after it was investigated under the DSA due to concerns about its \"addictive effect\", especially for children. A 2024 study of deleted Facebook and YouTube comments by the Future of Free Speech think tank at Vanderbilt University suggested that \"platforms, pages, or channels may be over-removing content to avoid regulatory penalties\" under the DSA. The Washington Post wrote in 2023 that tech companies may apply features instituted to comply with the DSA to countries outside of the EU, and that researchers have argued that the DSA could provide a framework for the United States to impose stricter regulations on tech companies. The Economist wrote in 2023 that the Brussels effect, whereby social media platforms implement EU regulations globally to save costs, \"is far from guaranteed\" with the DSA due to tech companies being unwilling to \"[lose] sovereignty over their digital territories everywhere\". Among legal academics, Dawn Nunziato of the George Washington University argued in 2022 that the DSA \"will further instantiate the Brussels Effect, whereby EU regulators wield powerful influence on how social media platforms moderate content on the global scale\". Suzanne Vergnolle of the Conservatoire national des arts et métiers stated her belief in 2023 that the DSA would have a Brussels effect, similar to that of the General Data Protection Regulation, but that \"it's going to take years\". Martin Husovec of the London School of Economics and Jennifer Urban of the University of California, Berkeley wrote in 2024 that \"the chances of spontaneous voluntary implementation beyond the EU's borders for four key parts of the DSA — content moderation procedures, transparency and governance obligations, and risk management rules — seem modest.\"\n\nThe 2023 Brazilian Fake News Bill, a proposed new social media regulation framework introduced in the National Congress, heavily referenced the DSA and contained similar provisions."}
{"title": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB976", "summary": "This bill, the Protecting Our Kids from Social Media Addiction Act, would make it unlawful for the operator of an addictive internet-based service or application, as defined, to provide an addictive feed to a user, unless the operator does not have actual knowledge that the user is a minor; commencing January 1, 2027, has reasonably determined that the user is not a minor; or has obtained verifiable parental consent to provide an addictive feed to the user who is a minor.", "content": "Senate Bill No. 976 CHAPTER 321 An act to add Chapter 24 (commencing with Section 27000) to Division 20 of the Health and Safety Code, relating to youth addiction. Existing law, the California Age-Appropriate Design Code Act, requires, beginning July 1, 2024, a business that provides an online service, product, or feature likely to be accessed by children to comply with certain requirements. The act requires the business to complete a data protection impact assessment addressing, among other things, whether the design could harm children and whether and how the online product, service, or feature uses system design features to increase, sustain, or extend use of the online product, service, or feature by children, including the automatic playing of media, rewards for time spent, and notifications. Existing law prohibits the business from using the personal information of any child in a way that the business knows, or has reason to know, is materially detrimental to the physical health, mental health, or well-being of a child. Existing law, the Privacy Rights for California Minors in the Digital World, prohibits an operator of an internet website, online service, online application, or mobile application from specified conduct when minors are involved, including the marketing or advertising of alcoholic beverages, firearms, or certain other products or services. Existing law sets forth other related protections for minors, including under the California Consumer Privacy Act of 2018 and the California Privacy Rights Act of 2020. This bill, the Protecting Our Kids from Social Media Addiction Act, would make it unlawful for the operator of an addictive internet-based service or application, as defined, to provide an addictive feed to a user, unless the operator does not have actual knowledge that the user is a minor; commencing January 1, 2027, has reasonably determined that the user is not a minor; or has obtained verifiable parental consent to provide an addictive feed to the user who is a minor. The bill would define “addictive feed” as an internet website, online service, online application, or mobile application, in which multiple pieces of media generated or shared by users are recommended, selected, or prioritized for display to a user based on information provided by the user, or otherwise associated with the user or the user’s device, as specified, unless any of certain conditions are met. The bill would make it unlawful for the operator of an addictive internet-based service or application, between the hours of 12 a.m. and 6 a.m., in the user’s local time zone, and between the hours of 8 a.m. and 3 p.m., Monday through Friday from September through May in the user’s local time zone, to send notifications to a user if the operator has actual knowledge that the user is a minor or, commencing January 1, 2027, has not reasonably determined that the user is not a minor, unless the operator has obtained verifiable parental consent to send those notifications, as specified. The bill would set forth related provisions for certain access controls determined by the verified parent through a mechanism provided by the operator. Under the bill, a parent’s provision of consent or use of a mechanism, as described above, would not waive, release, otherwise limit, or serve as a defense to, any claim that the parent, or that the user who is a minor or was a minor at the time of using the internet-based service or application, might have against the operator regarding any harm to the mental health or well-being of the user. The bill would require an operator to annually disclose the number of minor users of its addictive internet-based service or application, and of that total the number for whom the operator has received verifiable parental consent to provide an addictive feed, and the number of minor users as to whom the access controls are or are not enabled. Under the bill, these provisions would only be enforced in a civil action brought in the name of the people of the State of California by the Attorney General. The bill would require the Attorney General to adopt regulations to further the purposes of these provisions, including regulations regarding age assurance and parental consent by January 1, 2027. The bill would authorize the Attorney General to adopt regulations that provide for exceptions to these provisions, but only if those exceptions further the purpose of protecting minors. The bill would require the Attorney General, in promulgating regulations, to solicit public comment regarding the impact that any regulation might have based on certain nondiscrimination characteristics set forth in existing law. The bill would make these provisions severable. The people of the State of California do enact as follows: The Legislature finds and declares the following: Social media provides an important tool for communication and information sharing. Approximately 95 percent of 13- to 17-year-olds, inclusive, say that they use at least one social media platform, and more than one-third report using social media almost constantly. However, some social media platforms have evolved to include addictive features, including the algorithmic delivery of content and other design features, that pose a significant risk of harm to the mental health and well-being of children and adolescents. As the United States Surgeon General has reported, recent evidence has identified “reasons for concern” about social media usage by children and adolescents. This evidence includes a study concluding that the risk of poor mental health outcomes doubles for children and adolescents who use social media at least three hours a day and research finding that social media usage is linked to a variety of negative health outcomes, including low self-esteem and disordered eating, for adolescent girls. Heavier usage of social media also leads to less healthy sleep patterns and sleep quality, which can in turn exacerbate both physical and mental health problems. Further, social media usage is more strongly associated with negative mental health outcomes, including depressive symptoms and self-harm behaviors, than is consumption of other forms of media such as television or electronic games. Both California and the country as a whole are facing an ongoing youth mental health crisis, with rates of adolescent suicides, depressive episodes, and feelings of sadness and hopelessness on the rise in recent years. For these reasons, it is essential that California act to ensure that social media platforms obtain parental consent before exposing children and adolescents to harmful and addictive social media features. This chapter shall be known, and may be cited, as the Protecting Our Kids from Social Media Addiction Act. For purposes of this chapter, the following terms have the following meanings: “Addictive feed” means an internet website, online service, online application, or mobile application, or a portion thereof, in which multiple pieces of media generated or shared by users are, either concurrently or sequentially, recommended, selected, or prioritized for display to a user based, in whole or in part, on information provided by the user, or otherwise associated with the user or the user’s device, unless any of the following conditions are met, alone or in combination with one another: The information is not persistently associated with the user or user’s device, and does not concern the user’s previous interactions with media generated or shared by others. The information consists of search terms that are not persistently associated with the user or user’s device. The information consists of user-selected privacy or accessibility settings, technical information concerning the user’s device, or device communications or signals concerning whether the user is a minor. The user expressly and unambiguously requested the specific media or media by the author, creator, or poster of the media, or the blocking, prioritization, or deprioritization of such media, provided that the media is not recommended, selected, or prioritized for display based, in whole or in part, on other information associated with the user or the user’s device, except as otherwise permitted by this chapter and, in the case of audio or video content, is not automatically played. The media consists of direct, private communications between users. The media recommended, selected, or prioritized for display is exclusively the next media in a preexisting sequence from the same author, creator, poster, or source and, in the case of audio or video content, is not automatically played. The recommendation, selection, or prioritization of the media is necessary to comply with this chapter or any regulations promulgated pursuant to this chapter. “Addictive internet-based service or application” means an internet website, online service, online application, or mobile application, including, but not limited to, a “social media platform” as defined in Section 22675 of the Business and Professions Code, that offers users or provides users with an addictive feed as a significant part of the service provided by that internet website, online service, online application, or mobile application. “Addictive internet-based service or application” does not apply to either of the following: An internet website, online service, online application, or mobile application for which interactions between users are limited to commercial transactions or to consumer reviews of products, sellers, services, events, or places, or any combination thereof. An internet website, online service, online application, or mobile application that operates a feed for the primary purpose of cloud storage. “Media” means text, audio, an image, or a video. “Minor” means an individual under 18 years of age who is located in the State of California. “Operator” means a person who operates or provides an internet website, an online service, an online application, or a mobile application. “Parent” means a parent or guardian, including as defined in regulations promulgated pursuant to this chapter. “User” means a person who uses an internet website, online service, online application, or mobile application. “User” does not include the operator or a person acting as an agent of the operator. It shall be unlawful for the operator of an addictive internet-based service or application to provide an addictive feed to a user unless either of the following is met: Except as provided in subparagraph (B), the operator does not have actual knowledge that the user is a minor. Commencing January 1, 2027, the operator has reasonably determined that the user is not a minor, including pursuant to regulations promulgated by the Attorney General. The operator has obtained verifiable parental consent to provide an addictive feed to the user who is a minor. Information collected for the purpose of determining a user’s age or verifying parental consent pursuant to this chapter shall not be used for any purpose other than compliance with this chapter or with another applicable law. The information collected shall be deleted immediately after it is used to determine a user’s age or to verify parental consent, except as necessary to comply with state or federal law. Except as provided in paragraph (2), it shall be unlawful for the operator of an addictive internet-based service or application, between the hours of 12 a.m. and 6 a.m., in the user’s local time zone, and between the hours of 8 a.m. and 3 p.m., from Monday through Friday from September through May in the user’s local time zone, to send notifications to a user if the operator has actual knowledge that the user is a minor unless the operator has obtained verifiable parental consent to send those notifications. Commencing January 1, 2027, it shall be unlawful for the operator of an addictive internet-based service or application, between the hours of 12 a.m. and 6 a.m., in the user’s local time zone, and between the hours of 8 a.m. and 3 p.m., from Monday through Friday from September through May in the user’s local time zone, to send notifications to a user whom the operator has not reasonably determined is not a minor, including pursuant to regulations promulgated by the Attorney General, unless the operator has obtained verifiable parental consent to send those notifications. The operator of an addictive internet-based service or application shall provide a mechanism through which the verified parent of a user who is a minor may do any of the following: Prevent their child from accessing or receiving notifications from the addictive internet-based service or application between specific hours chosen by the parent. This setting shall be set by the operator as on by default, in a manner in which the child’s access is limited between the hours of 12 a.m. and 6 a.m., in the user’s local time zone. Limit their child’s access to any addictive feed from the addictive internet-based service or application to a length of time per day specified by the verified parent. This setting shall be set by the operator as on by default, in a manner in which the child’s access is limited to one hour per day unless modified by the verified parent. Limit their child’s ability to view the number of likes or other forms of feedback to pieces of media within an addictive feed. This setting shall be set by the operator as on by default. Require that the default feed provided to the child when entering the internet-based service or application be one in which pieces of media are not recommended, selected, or prioritized for display based on information provided by the user, or otherwise associated with the user or the user’s device, other than the user’s age or status as a minor. Set their child’s account to private mode, in a manner in which only users to whom the child is connected on the addictive internet-based service or application may view or respond to content posted by the child. This setting shall be set by the operator as on by default. This chapter shall not be construed as requiring the operator of an addictive internet-based service or application to give a parent any additional or special access to, or control over, the data or accounts of their child. This chapter shall not be construed as preventing any action taken in good faith to restrict access to, or availability of, media. An operator may choose not to provide services to minors. However, the operator of an addictive internet-based service or application shall not withhold, degrade, lower the quality of, or increase the price of, any product, service, or feature, other than as required by this chapter, due to a user or parent availing themselves of the rights provided by this chapter, or due to the protections required by this chapter. A parent’s provision of consent as described in Section 27001 or 27002, or the use by a parent of a mechanism as described in Section 27002, does not waive, release, otherwise limit, or serve as a defense to, any claim that the parent, or that the user who is a minor or was a minor at the time of using the internet-based service or application, might have against the operator of an addictive internet-based service or application regarding any harm to the mental health or well-being of the user. The protections provided by this chapter are in addition to those provided by any other applicable law, including, but not limited to, the California Age-Appropriate Design Code Act (Title 1.81.47 (commencing with Section 1798.99.28) of Part 4 of Division 3 of the Civil Code). An operator of an addictive internet-based service or application shall publicly disclose, on an annual basis, the number of minor users of its addictive internet-based service or application, and of that total the number for whom the operator has received verifiable parental consent to provide an addictive feed, and the number of minor users as to whom the controls set forth in Section 27002 are or are not enabled. This chapter may only be enforced in a civil action brought in the name of the people of the State of California by the Attorney General. The Attorney General shall adopt regulations to further the purposes of this chapter, including regulations regarding age assurance and parental consent by January 1, 2027. The Attorney General may adopt regulations that provide for exceptions to this chapter, but only if those exceptions further the purpose of protecting minors. In promulgating the regulations described in subdivision (b), the Attorney General shall solicit public comment regarding the impact that any regulation might have based on the nondiscrimination characteristics set forth in Section 51 of the Civil Code or in any other applicable law. If any provision of this chapter, or application thereof, to any person or circumstance is held invalid, that invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end the provisions of this chapter are declared to be severable."}
{"title": "https://www.flsenate.gov/Session/Bill/2024/3", "summary": "Online Protections for Minors; Requiring social media platforms to prohibit certain minors from creating new accounts; requiring social media platforms to terminate certain accounts and provide additional options for termination of such accounts; providing conditions under which social media platforms are required to prohibit certain minors from entering into contracts to become account holders; authorizing the Department of Legal Affairs to bring actions under the Florida Deceptive and Unfair Trade Practices Act for knowing or reckless violations; authorizing the department to issue and enforce civil investigative demands under certain circumstances, etc.", "content": "CS/CS/HB 3: Online Protections for Minors GENERAL BILL by Judiciary Committee; Regulatory Reform & Economic Development Subcommittee; Tramont; Overdorf; Sirois; McFarland; Rayner; (CO-INTRODUCERS) Anderson; Bankson; Barnaby; Beltran; Black; Botana; Brackett; Buchanan; Canady; Caruso; Chamberlin; Chambliss; Chaney; Fabricio; Garcia; Gonzalez Pittman; Gossett-Seidman; Gregory; Jacques; Leek; Lopez, V.; Massullo; McClain; Melo; Michael; Mooney; Payne; Persons-Mulicka; Plakon; Plasencia; Roth; Salzman; Snyder; Steele; Temple; Trabulsy; Truenow; Tuck; Waldron; Yarkosky; Yeager.\n\nOnline Protections for Minors; Requiring social media platforms to prohibit certain minors from creating new accounts; requiring social media platforms to terminate certain accounts and provide additional options for termination of such accounts; providing conditions under which social media platforms are required to prohibit certain minors from entering into contracts to become account holders; authorizing the Department of Legal Affairs to bring actions under the Florida Deceptive and Unfair Trade Practices Act for knowing or reckless violations; authorizing the department to issue and enforce civil investigative demands under certain circumstances, etc.\n\nEffective Date: 1/1/2025. Last Action: 3/25/2024 - Chapter No. 2024-42; companion bill(s) passed, see CS/CS/HB 1491 (Ch. 2024-54).\n\nOn 1/5/2024, the House filed the bill.\nOn 1/9/2024, the House referred it to the Regulatory Reform & Economic Development Subcommittee and to the Judiciary Committee, then added it to the Regulatory Reform & Economic Development Subcommittee agenda for 1st Reading (Original Filed Version).\nOn 1/11/2024, the House reported favorably with CS by the Regulatory Reform & Economic Development Subcommittee, laid it on the Table under Rule 7.18(a), filed the CS, and held a 1st Reading (Committee Substitute 1).\nOn 1/12/2024, the House referred it to the Judiciary Committee and added it to the Judiciary Committee agenda.\nOn 1/17/2024, the House reported favorably with CS by the Judiciary Committee.\nOn 1/18/2024, the House laid it on the Table under Rule 7.18(a), filed the CS, referred the bill to the House Calendar, added it to the Special Order Calendar (1/23/2024), and held a 1st Reading (Committee Substitute 2).\nOn 1/23/2024, the House read it a 2nd time, placed it on 3rd reading, and added it to the Third Reading Calendar.\nOn 1/24/2024, the House read it a 3rd time; CS passed with 119 YEAS and 0 NAYS.\nOn 1/24/2024, the Senate received it in Messages.\nOn 1/25/2024, the Senate referred it to Fiscal Policy and received it.\nOn 2/12/2024, the Senate placed it on the Committee agenda for Fiscal Policy, 02/15/24, 12:00 pm, 412 Knott Building, but it was Temporarily Postponed.\nOn 2/15/2024, the Senate placed it on the Special Order Calendar, 02/21/24, if received.\nOn 3/1/2024, the Senate withdrew it from Fiscal Policy -SJ 575, placed it on the Calendar on 2nd reading, and placed it on the Special Order Calendar, 03/04/24 -SJ 575.\nOn 3/4/2024, the Senate read it a 2nd time -SJ 622, adopted amendment(s) (961382) -SJ 622, read it a 3rd time -SJ 625, and the CS passed as amended with 30 YEAS and 5 NAYS -SJ 625.\nOn 3/4/2024, the House received it in Messages.\nOn 3/6/2024, the House added it to the Senate Message List, concurred with Amendment 961382, and the CS passed as amended with 109 YEAS and 4 NAYS. It was then ordered engrossed and enrolled.\nOn 3/21/2024, it was Signed by Officers and presented to the Governor.\nOn 3/25/2024, it was Approved by Governor, becoming Chapter No. 2024-42; companion bill(s) passed, see CS/CS/HB 1491 (Ch. 2024-54).\n\nH 1491 (er), Public Records/Investigations by the Department of Legal Affairs, was Linked by the Regulatory Reform & Economic Development Subcommittee. Its Last Action was 4/3/2024, Chapter No. 2024-54; companion bill(s) passed, see CS/CS/HB 3 (Ch. 2024-42).\nH 1 (er), Online Protections for Minors, was Similar and filed by the Judiciary Committee. Its Last Action was 3/1/2024, Vetoed by Governor; companion bill(s) passed, see CS/CS/HB 3 (Ch. 2024-42); companion bill(s) passed, see CS/CS/HB 1491 (Ch. 2024-2024-54).\nH 1377 (er), Public Records/Investigations by the Department of Legal Affairs, was Compared by the State Affairs Committee. Its Last Action was 3/1/2024, Vetoed by Governor; companion bill(s) passed, see CS/CS/HB 1491 (Ch. 2024-54), CS/CS/HB 3 (Ch. 2024-42).\nS 454, Protection of Minors on Social Media Platforms, was Compared by Garcia. Its Last Action was 3/8/2024, S Died in Commerce and Tourism, companion bill(s) passed, see CS/CS/HB 3 (Ch. 2024-42).\nS 1788 (c1), Social Media Use for Minors, was Compared by Grall. Its Last Action was 3/8/2024, S Died in Fiscal Policy, companion bill(s) passed, see CS/CS/HB 3 (Ch. 2024-42).\nS 1792 (c1), Online Access to Materials Harmful to Minors, was Compared by Grall. Its Last Action was 3/8/2024, S Died in Fiscal Policy, companion bill(s) passed, see CS/CS/HB 3 (Ch. 2024-42).\n\nAn Identical bill refers to companion bills that are identical word-for-word, not including titles. However, Resolutions and Concurrent Resolutions are considered identical if the only difference is the word \"House\" or \"Senate.\"\nA Similar bill refers to companion bills that are substantially similar in text or have substantial portions of text that are largely the same.\nA Compare bill refers to bills that have selected provisions that are similar in text.\nA Linked bill is a bill that is contingent upon passage of another bill within the same chamber, for example, a trust fund bill, a bill providing a public record exemption, or an implementing bill.\n\nThe page numbers, when listed, for citations are constantly under review. The journals or printed bills of the respective chambers should be consulted as the official documents of the Legislature. The links for the page numbers are formatted to open the bill text PDF directly to the page containing the citation. However, if your browser is set to open PDFs in a new window, as is often the case with 64-bit browsers, the bill text will open to the first page."}
{"title": "https://en.wikipedia.org/wiki/Utah_Social_Media_Regulation_Act", "summary": "S.B. 152 and H.B. 311, collectively known as the Utah Social Media Regulation Act, were social media bills that were passed by the Utah State Legislature in March 2023. The bills would've collectively imposed restrictions on how social networking services serve minors in the state of Utah, including mandatory age verification, and restrictions on data collection, algorithmic recommendations, and on when social networks would've been accessible to minors. The Act was intended to take effect in March 2024. However, following a lawsuit over the Act by NetChoice, the Utah attorney general stated in January 2024 that its implementation had been delayed to October 2024, but was likely to be repealed and amended. On September 10, 2024 Chief Judge Robert J. Shelby issued a written order granting a request from NetChoice, a tech industry group, for a preliminary injunction, meaning that Utah will be unable to enforce its social media law as litigation plays out. The law was appealed to the 10th Circuit on October 11, 2024 and is awaiting a decision.", "content": "S.B. 152 and H.B. 311, collectively known as the Utah Social Media Regulation Act, were social media bills that were passed by the Utah State Legislature in March 2023. The bills would've collectively imposed restrictions on how social networking services serve minors in the state of Utah, including mandatory age verification, and restrictions on data collection, algorithmic recommendations, and on when social networks would've been accessible to minors. The Act was intended to take effect in March 2024. However, following a lawsuit over the Act by NetChoice, the Utah attorney general stated in January 2024 that its implementation had been delayed to October 2024, but was likely to be repealed and amended. On September 10, 2024 Chief Judge Robert J. Shelby issued a written order granting a request from NetChoice, a tech industry group, for a preliminary injunction, meaning that Utah will be unable to enforce its social media law as litigation plays out. The law was appealed to the 10th Circuit on October 11, 2024 and is awaiting a decision.\n\nThe Act comprises two bills, S.B. 152 and H.B. 311, which respectively regulate access to social network accounts registered to minors, and impose obligations on social networking services to follow design practices that protect the privacy of minors. The bills would apply to social networks with more than 5 million active users in the United States. Social networking services would've verified the age of all users in the state of Utah, or else their account must've been deleted. The Act does not specify a specific method of age verification. Users who are under 18 must have consent from a parent or guardian to open an account, and the parent must be able to have access to the account and its data for monitoring. Unless required to comply with state or federal law, social networks were prohibited from collecting data based on the activity of minors, and may've not displayed targeted advertising or algorithmic recommendations of content, users, or groups to minors. A social network must not allow minors to access the service between the hours of 10:30 p.m., and 6:30 a.m. without parental consent. H.B. 311 prohibits social networks from exposing features to minors that cause them to have an \"addiction\" to the platform; the service must perform quarterly audits, and may be sued by users for harms caused by providing \"addictive\" features; there is a rebuttable presumption of harm if the plaintiff is 16 or younger. The bills prescribed fines of $2,500 per-violation for violations of the provisions of S.B. 152, and up to $250,000 in liabilities (plus fines of $2,500 per-user) for violations of the addiction rules.\n\nThe two bills were passed in early-March 2023, and signed by Governor Spencer Cox on March 23, 2023. Cox cited studies linking social media addiction to increases in depression and suicide among youth. They were originally intended to take effect on March 1, 2024. In the wake of a lawsuit in Arkansas by the trade association NetChoice over a similar bill, state senator and bill author Mike McKell stated that he planned to introduce amendments when the legislature resumed in 2024. In December 2023, NetChoice filed a lawsuit in Utah seeking to block the Act, citing that its definition of a social network was too vague, and that it \"restricts who can express themselves, what can be said, and when and how speech on covered websites can occur, down to the very hours of the day minors can use covered websites. The First Amendment, reinforced by decades of precedent, allows none of this.\" In regards to its age verification requirements, NetChoice argued that \"it may not be enough to simply verify the age of whatever person may be listed on a form of identification (even if they have such a record) because that record may not accurately reflect who the individual actually is.\" The office of the attorney general stated that the state was \"reviewing the lawsuit but remains intently focused on the goal of this legislation: Protecting young people from negative and harmful effects of social media use.\" In January 2024, Attorney General Sean Reyes asked the court to delay a hearing over the bill, stating that its effective date had been delayed to October 2024, and that the legislature planned to repeal and replace the bills."}
{"title": "https://www.law.cornell.edu/uscode/text/18/2258A", "summary": "In order to reduce the proliferation of online child sexual exploitation and to prevent the online sexual exploitation of children, a provider shall, as soon as reasonably possible after obtaining actual knowledge of any facts or circumstances described in paragraph (2)(A), take the actions described in subparagraph (B); and may, after obtaining actual knowledge of any facts or circumstances described in paragraph (2)(B), take the actions described in subparagraph (B).", "content": "In order to reduce the proliferation of online child sexual exploitation and to prevent the online sexual exploitation of children, a provider shall, as soon as reasonably possible after obtaining actual knowledge of any facts or circumstances described in paragraph (2)(A), take the actions described in subparagraph (B); and may, after obtaining actual knowledge of any facts or circumstances described in paragraph (2)(B), take the actions described in subparagraph (B). The actions described in this subparagraph are providing to the CyberTipline of NCMEC, or any successor to the CyberTipline operated by NCMEC, the mailing address, telephone number, facsimile number, electronic mailing address of, and individual point of contact for, such provider; and making a report of such facts or circumstances to the CyberTipline, or any successor to the CyberTipline operated by NCMEC. The facts or circumstances described in this subparagraph are any facts or circumstances from which there is an apparent violation of section 2251, 2251A, 2252, 2252A, 2252B, or 2260 that involves child pornography, of section 1591 (if the violation involves a minor), or of 2422(b). The facts or circumstances described in this subparagraph are any facts or circumstances which indicate a violation of any of the sections described in subparagraph (A) involving child pornography may be planned or imminent. In an effort to prevent the future sexual victimization of children, and to the extent the information is within the custody or control of a provider, the facts and circumstances included in each report under subsection (a)(1) may, at the sole discretion of the provider, include the following information: Information relating to the identity of any individual who appears to have violated or plans to violate a Federal law described in subsection (a)(2), which may, to the extent reasonably practicable, include the electronic mail address, Internet Protocol address, uniform resource locator, payment information (excluding personally identifiable information), or any other identifying information, including self-reported identifying information. Information relating to when and how a customer or subscriber of a provider uploaded, transmitted, or received content relating to the report or when and how content relating to the report was reported to, or discovered by the provider, including a date and time stamp and time zone. Information relating to the geographic location of the involved individual or website, which may include the Internet Protocol address or verified address, or, if not reasonably available, at least one form of geographic identifying information, including area code or zip code, provided by the customer or subscriber, or stored or obtained by the provider. Any visual depiction of apparent child pornography or other content relating to the incident such report is regarding. The complete communication containing any visual depiction of apparent child pornography or other content, including any data or information regarding the transmission of the communication; and any visual depictions, data, or other digital files contained in, or attached to, the communication. Pursuant to its clearinghouse role as a private, nonprofit organization, and at the conclusion of its review in furtherance of its nonprofit mission, NCMEC shall make available each report made under subsection (a)(1) to one or more of the following law enforcement agencies: Any Federal law enforcement agency that is involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes. Any State or local law enforcement agency that is involved in the investigation of child sexual exploitation. A foreign law enforcement agency designated by the Attorney General under subsection (d)(3) or a foreign law enforcement agency that has an established relationship with the Federal Bureau of Investigation, Immigration and Customs Enforcement, or INTERPOL, and is involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes. The Attorney General shall enforce this section. The Attorney General may designate a Federal law enforcement agency or agencies to which a report shall be forwarded under subsection (c)(1). The Attorney General may, in consultation with the Secretary of State, designate foreign law enforcement agencies to which a report may be forwarded under subsection (c)(3); establish the conditions under which such a report may be forwarded to such agencies; and develop a process for foreign law enforcement agencies to request assistance from Federal law enforcement agencies in obtaining evidence related to a report referred under subsection (c)(3). The Attorney General may maintain and make available to the Department of State, NCMEC, providers, the Committee on the Judiciary of the Senate, and the Committee on the Judiciary of the House of Representatives a list of the foreign law enforcement agencies designated under paragraph (3). NCMEC may notify a provider of the information described in subparagraph (B), if a provider notifies NCMEC that the provider is making a report under this section as the result of a request by a foreign law enforcement agency; and NCMEC forwards the report described in clause (i) to the requesting foreign law enforcement agency; or another agency in the same country designated by the Attorney General under paragraph (3) or that has an established relationship with the Federal Bureau of Investigation, U.S. Immigration and Customs Enforcement, or INTERPOL and is involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes. The information described in this subparagraph is the identity of the foreign law enforcement agency to which the report was forwarded; and the date on which the report was forwarded. If a provider notifies NCMEC that the provider is making a report under this section as the result of a request by a foreign law enforcement agency and NCMEC is unable to forward the report as described in subparagraph (A)(ii), NCMEC shall notify the provider that NCMEC was unable to forward the report. A provider that knowingly and willfully fails to make a report required under subsection (a)(1) shall be fined in the case of an initial knowing and willful failure to make a report, not more than $850,000 in the case of a provider with not less than 100,000,000 monthly active users or $600,000 in the case of a provider with less than 100,000,000 monthly active users; and in the case of any second or subsequent knowing and willful failure to make a report, not more than $1,000,000 in the case of a provider with not less than 100,000,000 monthly active users or $850,000 in the case of a provider with less than 100,000,000 monthly active users. Nothing in this section shall be construed to require a provider to monitor any user, subscriber, or customer of that provider; monitor the content of any communication of any person described in paragraph (1); or affirmatively search, screen, or scan for facts or circumstances described in sections (a) and (b). Except as provided in paragraph (2), a law enforcement agency that receives a report under subsection (c) shall not disclose any information contained in that report. A law enforcement agency may disclose information in a report received under subsection (c) to an attorney for the government for use in the performance of the official duties of that attorney; to such officers and employees of that law enforcement agency, as may be necessary in the performance of their investigative and recordkeeping functions; to such other government personnel (including personnel of a State or subdivision of a State) as are determined to be necessary by an attorney for the government to assist the attorney in the performance of the official duties of the attorney in enforcing Federal criminal law; if the report discloses a violation of State criminal law, to an appropriate official of a State or subdivision of a State for the purpose of enforcing such State law; to a defendant in a criminal case or the attorney for that defendant, subject to the terms and limitations under section 3509(m) or a similar State law, to the extent the information relates to a criminal charge pending against that defendant; subject to subparagraph (B), to a provider if necessary to facilitate response to legal process issued in connection to a criminal investigation, prosecution, or post-conviction remedy relating to that report; and as ordered by a court upon a showing of good cause and pursuant to any protective orders or other conditions that the court may impose. Nothing in subparagraph (A)(vi) authorizes a law enforcement agency to provide visual depictions of apparent child pornography to a provider. NCMEC may disclose by mail, electronic transmission, or other reasonable means, information received in a report under subsection (a) only to any Federal law enforcement agency designated by the Attorney General under subsection (d)(2) or that is involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes; any State, local, or tribal law enforcement agency involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes; any foreign law enforcement agency designated by the Attorney General under subsection (d)(3) or that has an established relationship with the Federal Bureau of Investigation, Immigration and Customs Enforcement, or INTERPOL, and is involved in the investigation of child sexual exploitation, kidnapping, or enticement crimes; a provider as described in section 2258C; and respond to legal process, as necessary. A provider that submits a report under subsection (a)(1) may disclose by mail, electronic transmission, or other reasonable means, information, including visual depictions contained in the report, in a manner consistent with permitted disclosures under paragraphs (3) through (8) of section 2702(b) only to a law enforcement agency described in subparagraph (A), (B), or (C) of paragraph (3), to NCMEC, or as necessary to respond to legal process. For the purposes of this section, a completed submission by a provider of a report to the CyberTipline under subsection (a)(1) shall be treated as a request to preserve the contents provided in the report for 1 year after the submission to the CyberTipline. Pursuant to paragraph (1), a provider shall preserve any visual depictions, data, or other digital files that are reasonably accessible and may provide context or additional information about the reported material or person. A provider preserving materials under this section shall maintain the materials in a secure location and take appropriate steps to limit access by agents or employees of the service to the materials to that access necessary to comply with the requirements of this subsection. Nothing in this section shall be construed as replacing, amending, or otherwise interfering with the authorities and duties under section 2703. A provider of a report to the CyberTipline under subsection (a)(1) may voluntarily preserve the contents provided in the report (including any comingled content described in paragraph (2)) for longer than 1 year after the submission to the CyberTipline for the purpose of reducing the proliferation of online child sexual exploitation or preventing the online sexual exploitation of children. Not later than 1 year after the date of enactment of this paragraph, a provider of a report to the CyberTipline under subsection (a)(1) shall preserve materials under this subsection in a manner that is consistent with the most recent version of the Cybersecurity Framework developed by the National Institute of Standards and Technology, or any successor thereto."}
